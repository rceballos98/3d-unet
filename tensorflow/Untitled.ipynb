{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from my_tf_unet import util\n",
    "from my_tf_unet.layers import (weight_variable, weight_variable_devonc, bias_variable, conv3d, deconv3d, max_pool, crop_and_concat, pixel_wise_softmax_2,cross_entropy)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2, summaries=True):\n",
    "    \"\"\"\n",
    "    Creates a new convolutional unet for the given parametrization.\n",
    "    \n",
    "    :param x: input tensor, shape [?,nx,ny,channels]\n",
    "    :param keep_prob: dropout probability tensor\n",
    "    :param channels: number of channels in the input image\n",
    "    :param n_class: number of output labels\n",
    "    :param layers: number of layers in the net\n",
    "    :param features_root: number of features in the first layer\n",
    "    :param filter_size: size of the convolution filter\n",
    "    :param pool_size: size of the max pooling operation\n",
    "    :param summaries: Flag if summaries should be created\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Layers {layers}, features {features}, filter size {filter_size}x{filter_size}, pool size: {pool_size}x{pool_size}\".format(layers=layers,\n",
    "                                                                                                           features=features_root,\n",
    "                                                                                                           filter_size=filter_size,\n",
    "                                                                                                           pool_size=pool_size))\n",
    "    # Placeholder for the input image\n",
    "    nx = tf.shape(x)[1]\n",
    "    ny = tf.shape(x)[2]\n",
    "    x_image = tf.reshape(x, tf.stack([-1,nx,ny,channels]))\n",
    "    in_node = x_image\n",
    "    batch_size = tf.shape(x_image)[0]\n",
    " \n",
    "    weights = []\n",
    "    biases = []\n",
    "    convs = []\n",
    "    pools = OrderedDict()\n",
    "    deconv = OrderedDict()\n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "    \n",
    "    in_size = 1000\n",
    "    size = in_size\n",
    "    # down layers\n",
    "    for layer in range(0, layers):\n",
    "        features = 2**layer*features_root\n",
    "        stddev = np.sqrt(2 / (filter_size**2 * features))\n",
    "        if layer == 0:\n",
    "            w1 = weight_variable([filter_size, filter_size, channels, features], stddev)\n",
    "        else:\n",
    "            w1 = weight_variable([filter_size, filter_size, features//2, features], stddev)\n",
    "            \n",
    "        w2 = weight_variable([filter_size, filter_size, features, features], stddev)\n",
    "        b1 = bias_variable([features])\n",
    "        b2 = bias_variable([features])\n",
    "        \n",
    "        conv1 = conv3d(in_node, w1, keep_prob)\n",
    "        tmp_h_conv = tf.nn.relu(conv1 + b1)\n",
    "        conv2 = conv3d(tmp_h_conv, w2, keep_prob)\n",
    "        dw_h_convs[layer] = tf.nn.relu(conv2 + b2)\n",
    "        \n",
    "        weights.append((w1, w2))\n",
    "        biases.append((b1, b2))\n",
    "        convs.append((conv1, conv2))\n",
    "        \n",
    "        size -= 4\n",
    "        if layer < layers-1:\n",
    "            pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "            in_node = pools[layer]\n",
    "            size /= 2\n",
    "        \n",
    "    in_node = dw_h_convs[layers-1]\n",
    "        \n",
    "    # up layers\n",
    "    for layer in range(layers-2, -1, -1):\n",
    "        features = 2**(layer+1)*features_root\n",
    "        stddev = np.sqrt(2 / (filter_size**2 * features))\n",
    "        \n",
    "        wd = weight_variable_devonc([pool_size, pool_size, features//2, features], stddev)\n",
    "        bd = bias_variable([features//2])\n",
    "        h_deconv = tf.nn.relu(deconv3d(in_node, wd, pool_size) + bd)\n",
    "        h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)\n",
    "        deconv[layer] = h_deconv_concat\n",
    "        \n",
    "        w1 = weight_variable([filter_size, filter_size, features, features//2], stddev)\n",
    "        w2 = weight_variable([filter_size, filter_size, features//2, features//2], stddev)\n",
    "        b1 = bias_variable([features//2])\n",
    "        b2 = bias_variable([features//2])\n",
    "        \n",
    "        conv1 = conv3d(h_deconv_concat, w1, keep_prob)\n",
    "        h_conv = tf.nn.relu(conv1 + b1)\n",
    "        conv2 = conv3d(h_conv, w2, keep_prob)\n",
    "        in_node = tf.nn.relu(conv2 + b2)\n",
    "        up_h_convs[layer] = in_node\n",
    "\n",
    "        weights.append((w1, w2))\n",
    "        biases.append((b1, b2))\n",
    "        convs.append((conv1, conv2))\n",
    "        \n",
    "        size *= 2\n",
    "        size -= 4\n",
    "\n",
    "    # Output Map\n",
    "    weight = weight_variable([1, 1, features_root, n_class], stddev)\n",
    "    bias = bias_variable([n_class])\n",
    "    conv = conv3d(in_node, weight, tf.constant(1.0))\n",
    "    output_map = tf.nn.relu(conv + bias)\n",
    "    up_h_convs[\"out\"] = output_map\n",
    "    \n",
    "    if summaries:\n",
    "        for i, (c1, c2) in enumerate(convs):\n",
    "            tf.summary.image('summary_conv_%02d_01'%i, get_image_summary(c1))\n",
    "            tf.summary.image('summary_conv_%02d_02'%i, get_image_summary(c2))\n",
    "            \n",
    "        for k in pools.keys():\n",
    "            tf.summary.image('summary_pool_%02d'%k, get_image_summary(pools[k]))\n",
    "        \n",
    "        for k in deconv.keys():\n",
    "            tf.summary.image('summary_deconv_concat_%02d'%k, get_image_summary(deconv[k]))\n",
    "            \n",
    "        for k in dw_h_convs.keys():\n",
    "            tf.summary.histogram(\"dw_convolution_%02d\"%k + '/activations', dw_h_convs[k])\n",
    "\n",
    "        for k in up_h_convs.keys():\n",
    "            tf.summary.histogram(\"up_convolution_%s\"%k + '/activations', up_h_convs[k])\n",
    "            \n",
    "    variables = []\n",
    "    for w1,w2 in weights:\n",
    "        variables.append(w1)\n",
    "        variables.append(w2)\n",
    "        \n",
    "    for b1,b2 in biases:\n",
    "        variables.append(b1)\n",
    "        variables.append(b2)\n",
    "\n",
    "    \n",
    "    return output_map, variables, int(in_size - size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unet(object):\n",
    "    \"\"\"\n",
    "    A unet implementation\n",
    "    \n",
    "    :param channels: (optional) number of channels in the input image\n",
    "    :param n_class: (optional) number of output labels\n",
    "    :param cost: (optional) name of the cost function. Default is 'cross_entropy'\n",
    "    :param cost_kwargs: (optional) kwargs passed to the cost function. See Unet._get_cost for more options\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels=3, n_class=2, cost=\"cross_entropy\", cost_kwargs={}, **kwargs):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.n_class = n_class\n",
    "        self.summaries = kwargs.get(\"summaries\", True)\n",
    "        \n",
    "        self.x = tf.placeholder(\"float\", shape=[None, None, None, channels])\n",
    "        self.y = tf.placeholder(\"float\", shape=[None, None, None, n_class])\n",
    "        self.keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "        \n",
    "        logits, self.variables, self.offset = create_conv_net(self.x, self.keep_prob, channels, n_class, **kwargs)\n",
    "        \n",
    "        self.cost = self._get_cost(logits, cost, cost_kwargs)\n",
    "        \n",
    "        self.gradients_node = tf.gradients(self.cost, self.variables)\n",
    "         \n",
    "        self.cross_entropy = tf.reduce_mean(cross_entropy(tf.reshape(self.y, [-1, n_class]),\n",
    "                                                          tf.reshape(pixel_wise_softmax_2(logits), [-1, n_class])))\n",
    "        \n",
    "        self.predicter = pixel_wise_softmax_2(logits)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.predicter, 3), tf.argmax(self.y, 3))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "        \n",
    "    def _get_cost(self, logits, cost_name, cost_kwargs):\n",
    "        \"\"\"\n",
    "        Constructs the cost function, either cross_entropy, weighted cross_entropy or dice_coefficient.\n",
    "        Optional arguments are: \n",
    "        class_weights: weights for the different classes in case of multi-class imbalance\n",
    "        regularizer: power of the L2 regularizers added to the loss function\n",
    "        \"\"\"\n",
    "        \n",
    "        flat_logits = tf.reshape(logits, [-1, self.n_class])\n",
    "        flat_labels = tf.reshape(self.y, [-1, self.n_class])\n",
    "        if cost_name == \"cross_entropy\":\n",
    "            class_weights = cost_kwargs.pop(\"class_weights\", None)\n",
    "            \n",
    "            if class_weights is not None:\n",
    "                class_weights = tf.constant(np.array(class_weights, dtype=np.float32))\n",
    "        \n",
    "                weight_map = tf.multiply(flat_labels, class_weights)\n",
    "                weight_map = tf.reduce_sum(weight_map, axis=1)\n",
    "        \n",
    "                loss_map = tf.nn.softmax_cross_entropy_with_logits(flat_logits, flat_labels)\n",
    "                weighted_loss = tf.multiply(loss_map, weight_map)\n",
    "        \n",
    "                loss = tf.reduce_mean(weighted_loss)\n",
    "                \n",
    "            else:\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_logits, \n",
    "                                                                              labels=flat_labels))\n",
    "        elif cost_name == \"dice_coefficient\":\n",
    "            eps = 1e-5\n",
    "            prediction = pixel_wise_softmax_2(logits)\n",
    "            intersection = tf.reduce_sum(prediction * self.y)\n",
    "            union =  eps + tf.reduce_sum(prediction) + tf.reduce_sum(self.y)\n",
    "            loss = -(2 * intersection/ (union))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown cost function: \"%cost_name)\n",
    "\n",
    "        regularizer = cost_kwargs.pop(\"regularizer\", None)\n",
    "        if regularizer is not None:\n",
    "            regularizers = sum([tf.nn.l2_loss(variable) for variable in self.variables])\n",
    "            loss += (regularizer * regularizers)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def predict(self, model_path, x_test):\n",
    "        \"\"\"\n",
    "        Uses the model to create a prediction for the given data\n",
    "        \n",
    "        :param model_path: path to the model checkpoint to restore\n",
    "        :param x_test: Data to predict on. Shape [n, nx, ny, channels]\n",
    "        :returns prediction: The unet prediction Shape [n, px, py, labels] (px=nx-self.offset/2) \n",
    "        \"\"\"\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variables\n",
    "            sess.run(init)\n",
    "        \n",
    "            # Restore model weights from previously saved model\n",
    "            self.restore(sess, model_path)\n",
    "            \n",
    "            y_dummy = np.empty((x_test.shape[0], x_test.shape[1], x_test.shape[2], self.n_class))\n",
    "            prediction = sess.run(self.predicter, feed_dict={self.x: x_test, self.y: y_dummy, self.keep_prob: 1.})\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "    def save(self, sess, model_path):\n",
    "        \"\"\"\n",
    "        Saves the current session to a checkpoint\n",
    "        \n",
    "        :param sess: current session\n",
    "        :param model_path: path to file system location\n",
    "        \"\"\"\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        return save_path\n",
    "    \n",
    "    def restore(self, sess, model_path):\n",
    "        \"\"\"\n",
    "        Restores a session from a checkpoint\n",
    "        \n",
    "        :param sess: current session instance\n",
    "        :param model_path: path to file system checkpoint location\n",
    "        \"\"\"\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        logging.info(\"Model restored from file: %s\" % model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a unet instance\n",
    "    \n",
    "    :param net: the unet instance to train\n",
    "    :param batch_size: size of training batch\n",
    "    :param optimizer: (optional) name of the optimizer to use (momentum or adam)\n",
    "    :param opt_kwargs: (optional) kwargs passed to the learning rate (momentum opt) and to the optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_path = \"prediction\"\n",
    "    verification_batch_size = 4\n",
    "    \n",
    "    def __init__(self, net, batch_size=1, optimizer=\"momentum\", opt_kwargs={}):\n",
    "        self.net = net\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.opt_kwargs = opt_kwargs\n",
    "        \n",
    "    def _get_optimizer(self, training_iters, global_step):\n",
    "        if self.optimizer == \"momentum\":\n",
    "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.2)\n",
    "            decay_rate = self.opt_kwargs.pop(\"decay_rate\", 0.95)\n",
    "            momentum = self.opt_kwargs.pop(\"momentum\", 0.2)\n",
    "            \n",
    "            self.learning_rate_node = tf.train.exponential_decay(learning_rate=learning_rate, \n",
    "                                                        global_step=global_step, \n",
    "                                                        decay_steps=training_iters,  \n",
    "                                                        decay_rate=decay_rate, \n",
    "                                                        staircase=True)\n",
    "            \n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate_node, momentum=momentum,\n",
    "                                                   **self.opt_kwargs).minimize(self.net.cost, \n",
    "                                                                                global_step=global_step)\n",
    "        elif self.optimizer == \"adam\":\n",
    "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.001)\n",
    "            self.learning_rate_node = tf.Variable(learning_rate)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_node, \n",
    "                                               **self.opt_kwargs).minimize(self.net.cost,\n",
    "                                                                     global_step=global_step)\n",
    "        \n",
    "        return optimizer\n",
    "        \n",
    "    def _initialize(self, training_iters, output_path, restore):\n",
    "        global_step = tf.Variable(0)\n",
    "        \n",
    "        self.norm_gradients_node = tf.Variable(tf.constant(0.0, shape=[len(self.net.gradients_node)]))\n",
    "        \n",
    "        if self.net.summaries:\n",
    "            tf.summary.histogram('norm_grads', self.norm_gradients_node)\n",
    "\n",
    "        tf.summary.scalar('loss', self.net.cost)\n",
    "        tf.summary.scalar('cross_entropy', self.net.cross_entropy)\n",
    "        tf.summary.scalar('accuracy', self.net.accuracy)\n",
    "\n",
    "        self.optimizer = self._get_optimizer(training_iters, global_step)\n",
    "        tf.summary.scalar('learning_rate', self.learning_rate_node)\n",
    "\n",
    "        self.summary_op = tf.summary.merge_all()        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        prediction_path = os.path.abspath(self.prediction_path)\n",
    "        output_path = os.path.abspath(output_path)\n",
    "        \n",
    "        if not restore:\n",
    "            logging.info(\"Removing '{:}'\".format(prediction_path))\n",
    "            shutil.rmtree(prediction_path, ignore_errors=True)\n",
    "            logging.info(\"Removing '{:}'\".format(output_path))\n",
    "            shutil.rmtree(output_path, ignore_errors=True)\n",
    "        \n",
    "        if not os.path.exists(prediction_path):\n",
    "            logging.info(\"Allocating '{:}'\".format(prediction_path))\n",
    "            os.makedirs(prediction_path)\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            logging.info(\"Allocating '{:}'\".format(output_path))\n",
    "            os.makedirs(output_path)\n",
    "        \n",
    "        return init\n",
    "\n",
    "    def train(self, data_provider, output_path, training_iters=10, epochs=100, dropout=0.75, display_step=1, restore=False):\n",
    "        \"\"\"\n",
    "        Lauches the training process\n",
    "        \n",
    "        :param data_provider: callable returning training and verification data\n",
    "        :param output_path: path where to store checkpoints\n",
    "        :param training_iters: number of training mini batch iteration\n",
    "        :param epochs: number of epochs\n",
    "        :param dropout: dropout probability\n",
    "        :param display_step: number of steps till outputting stats\n",
    "        :param restore: Flag if previous model should be restored \n",
    "        \"\"\"\n",
    "        save_path = os.path.join(output_path, \"model.cpkt\")\n",
    "        if epochs == 0:\n",
    "            return save_path\n",
    "        \n",
    "        init = self._initialize(training_iters, output_path, restore)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            if restore:\n",
    "                ckpt = tf.train.get_checkpoint_state(output_path)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.net.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            test_x, test_y = data_provider(self.verification_batch_size)\n",
    "            pred_shape = self.store_prediction(sess, test_x, test_y, \"_init\")\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter(output_path, graph=sess.graph)\n",
    "            logging.info(\"Start optimization\")\n",
    "            \n",
    "            avg_gradients = None\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                for step in range((epoch*training_iters), ((epoch+1)*training_iters)):\n",
    "                    batch_x, batch_y = data_provider(self.batch_size)\n",
    "                     \n",
    "                    # Run optimization op (backprop)\n",
    "                    _, loss, lr, gradients = sess.run((self.optimizer, self.net.cost, self.learning_rate_node, self.net.gradients_node), \n",
    "                                                      feed_dict={self.net.x: batch_x,\n",
    "                                                                 self.net.y: util.crop_to_shape(batch_y, pred_shape),\n",
    "                                                                 self.net.keep_prob: dropout})\n",
    "\n",
    "                    if avg_gradients is None:\n",
    "                        avg_gradients = [np.zeros_like(gradient) for gradient in gradients]\n",
    "                    for i in range(len(gradients)):\n",
    "                        avg_gradients[i] = (avg_gradients[i] * (1.0 - (1.0 / (step+1)))) + (gradients[i] / (step+1))\n",
    "                        \n",
    "                    norm_gradients = [np.linalg.norm(gradient) for gradient in avg_gradients]\n",
    "                    self.norm_gradients_node.assign(norm_gradients).eval()\n",
    "                    \n",
    "                    if step % display_step == 0:\n",
    "                        self.output_minibatch_stats(sess, summary_writer, step, batch_x, util.crop_to_shape(batch_y, pred_shape))\n",
    "                        \n",
    "                    total_loss += loss\n",
    "\n",
    "                self.output_epoch_stats(epoch, total_loss, training_iters, lr)\n",
    "                self.store_prediction(sess, test_x, test_y, \"epoch_%s\"%epoch)\n",
    "                    \n",
    "                save_path = self.net.save(sess, save_path)\n",
    "            logging.info(\"Optimization Finished!\")\n",
    "            \n",
    "            return save_path\n",
    "        \n",
    "    def store_prediction(self, sess, batch_x, batch_y, name):\n",
    "        prediction = sess.run(self.net.predicter, feed_dict={self.net.x: batch_x, \n",
    "                                                             self.net.y: batch_y, \n",
    "                                                             self.net.keep_prob: 1.})\n",
    "        pred_shape = prediction.shape\n",
    "        \n",
    "        loss = sess.run(self.net.cost, feed_dict={self.net.x: batch_x, \n",
    "                                                       self.net.y: util.crop_to_shape(batch_y, pred_shape), \n",
    "                                                       self.net.keep_prob: 1.})\n",
    "        \n",
    "        logging.info(\"Verification error= {:.1f}%, loss= {:.4f}\".format(error_rate(prediction,\n",
    "                                                                          util.crop_to_shape(batch_y,\n",
    "                                                                                             prediction.shape)),\n",
    "                                                                          loss))\n",
    "              \n",
    "        img = util.combine_img_prediction(batch_x, batch_y, prediction)\n",
    "        util.save_image(img, \"%s/%s.jpg\"%(self.prediction_path, name))\n",
    "        \n",
    "        return pred_shape\n",
    "    \n",
    "    def output_epoch_stats(self, epoch, total_loss, training_iters, lr):\n",
    "        logging.info(\"Epoch {:}, Average loss: {:.4f}, learning rate: {:.4f}\".format(epoch, (total_loss / training_iters), lr))\n",
    "    \n",
    "    def output_minibatch_stats(self, sess, summary_writer, step, batch_x, batch_y):\n",
    "        # Calculate batch loss and accuracy\n",
    "        summary_str, loss, acc, predictions = sess.run([self.summary_op, \n",
    "                                                            self.net.cost, \n",
    "                                                            self.net.accuracy, \n",
    "                                                            self.net.predicter], \n",
    "                                                           feed_dict={self.net.x: batch_x,\n",
    "                                                                      self.net.y: batch_y,\n",
    "                                                                      self.net.keep_prob: 1.})\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "        logging.info(\"Iter {:}, Minibatch Loss= {:.4f}, Training Accuracy= {:.4f}, Minibatch error= {:.1f}%\".format(step,\n",
    "                                                                                                            loss,\n",
    "                                                                                                            acc,\n",
    "                                                                                                            error_rate(predictions, batch_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"\n",
    "    Return the error rate based on dense predictions and 1-hot labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        np.sum(np.argmax(predictions, 3) == np.argmax(labels, 3)) /\n",
    "        (predictions.shape[0]*predictions.shape[1]*predictions.shape[2]))\n",
    "\n",
    "\n",
    "def get_image_summary(img, idx=0):\n",
    "    \"\"\"\n",
    "    Make an image summary for 4d tensor image with index idx\n",
    "    \"\"\"\n",
    "    \n",
    "    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
    "    V -= tf.reduce_min(V)\n",
    "    V /= tf.reduce_max(V)\n",
    "    V *= 255\n",
    "    \n",
    "    img_w = tf.shape(img)[1]\n",
    "    img_h = tf.shape(img)[2]\n",
    "    V = tf.reshape(V, tf.stack((img_w, img_h, 1)))\n",
    "    V = tf.transpose(V, (2, 0, 1))\n",
    "    V = tf.reshape(V, tf.stack((-1, img_w, img_h, 1)))\n",
    "    return V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ceNeuron]",
   "language": "python",
   "name": "conda-env-ceNeuron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
